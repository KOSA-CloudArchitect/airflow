from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.apache.kafka.sensors.kafka import AwaitMessageSensor
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.models.xcom_arg import XComArg

from datetime import datetime, timedelta
import logging
import json
import pytz
import os
from kafka import KafkaProducer
from kafka.errors import KafkaError

def publish_summary_request(**context):
    """Overall Summary Request Topicì— ë©”ì‹œì§€ ë°œí–‰"""
    dag_run = context.get("dag_run")
    conf = dag_run.conf or {}
    
    # Redshift COPY DAGì—ì„œ ì „ë‹¬ë°›ì€ ë°ì´í„° ì¶”ì¶œ
    job_id = conf.get('job_id', 'unknown')
    execution_time = conf.get('execution_time', '')
    copy_completion_time = conf.get('copy_completion_time', '')
    source_dag = conf.get('source_dag', 'unknown')
    
    # í•œêµ­ ì‹œê°„ìœ¼ë¡œ í˜„ì¬ ì‹œê°„ ê³„ì‚°
    kst = pytz.timezone('Asia/Seoul')
    current_time_kst = datetime.now(kst)
    
    # Overall Summary Request ë©”ì‹œì§€ êµ¬ì„±
    summary_request_message = {
        "job_id": job_id,
        "request_type": "overall_summary",
        "execution_time": execution_time,  # realtime_pipeline_monitor_dag ì‹¤í–‰ ì‹œê°„
        "copy_completion_time": copy_completion_time,  # Redshift COPY ì™„ë£Œ ì‹œê°„
        "request_timestamp": current_time_kst.isoformat(),
        "metadata": {
            "source_dag": source_dag,
            "trigger_point": "redshift_copy_completed",
            "analysis_type": "llm_summary",
            "worker_id": "airflow-summary-worker",
            "pipeline_execution_time": execution_time  # ì›ë³¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œê°„
        }
    }
    
    logging.info(f"[Summary Request] Publishing message for job_id={job_id}")
    logging.info(f"[Summary Request] Execution time from pipeline: {execution_time}")
    logging.info(f"[Summary Request] Copy completion time: {copy_completion_time}")
    logging.info(f"[Summary Request] Message: {summary_request_message}")
    
    # Kafka Producerë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë©”ì‹œì§€ ë°œí–‰
    try:
        # Kafka ì„¤ì •
        kafka_servers = os.getenv("KAFKA_BOOTSTRAP_SERVERS")
        topic_name = "overall-summary-request-topic"
        
        if not kafka_servers:
            raise ValueError("KAFKA_BOOTSTRAP_SERVERS í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        logging.info(f"[Summary Request] Connecting to Kafka: {kafka_servers}")
        logging.info(f"[Summary Request] Publishing to topic: {topic_name}")
        
        # Kafka Producer ì´ˆê¸°í™”
        producer = KafkaProducer(
            bootstrap_servers=kafka_servers,
            value_serializer=lambda v: json.dumps(v).encode("utf-8"),
            retries=3,
            acks='all',
            request_timeout_ms=30000,
            delivery_timeout_ms=120000
        )
        
        # ë©”ì‹œì§€ ì „ì†¡
        future = producer.send(topic_name, summary_request_message)
        
        # ì „ì†¡ ì™„ë£Œ ëŒ€ê¸°
        record_metadata = future.get(timeout=30)
        
        # Producer ì •ë¦¬
        producer.flush()
        producer.close()
        
        logging.info(f"[Summary Request] âœ… Message published successfully!")
        logging.info(f"[Summary Request] Topic: {record_metadata.topic}")
        logging.info(f"[Summary Request] Partition: {record_metadata.partition}")
        logging.info(f"[Summary Request] Offset: {record_metadata.offset}")
        
        print(f"ğŸ“¤ âœ… Successfully published to {topic_name}: {json.dumps(summary_request_message, indent=2)}", flush=True)
        
    except KafkaError as e:
        error_msg = f"Kafka ë©”ì‹œì§€ ë°œí–‰ ì‹¤íŒ¨: {str(e)}"
        logging.error(f"[Summary Request] âŒ {error_msg}")
        print(f"ğŸ“¤ âŒ Kafka publishing failed: {error_msg}", flush=True)
        raise e
    except Exception as e:
        error_msg = f"ë©”ì‹œì§€ ë°œí–‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}"
        logging.error(f"[Summary Request] âŒ {error_msg}")
        print(f"ğŸ“¤ âŒ Unexpected error: {error_msg}", flush=True)
        raise e
    
    # XComì— ë©”ì‹œì§€ ì €ì¥ (ë””ë²„ê¹…ìš©)
    context['task_instance'].xcom_push(key='summary_request_message', value=summary_request_message)
    
    return summary_request_message

def handle_summary_failure(context):
    """ìš”ì•½ ë¶„ì„ ì‹¤íŒ¨ ì²˜ë¦¬ í•¨ìˆ˜"""
    dag_run = context.get("dag_run")
    job_id = (dag_run.conf or {}).get("job_id") if dag_run else None
    task_id = context.get("task_instance").task_id
    
    logging.error(f"Summary analysis failed for job {job_id} at task {task_id}")
    print(f"ğŸš¨ Summary analysis failure: Job {job_id} failed at {task_id}", flush=True)
    
    # Discord/Slack ì•Œë¦¼ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
    # send_summary_failure_alert(context=context, job_id=job_id, task_id=task_id)

def notify_summary_completion(**context):
    """ìš”ì•½ ë¶„ì„ ì™„ë£Œ ì•Œë¦¼"""
    dag_run = context.get("dag_run")
    job_id = (dag_run.conf or {}).get("job_id") if dag_run else None
    
    logging.info(f"Summary analysis completed successfully for job {job_id}")
    print(f"ğŸ‰ Summary analysis completed for job {job_id}!", flush=True)
    
    return f"Summary analysis completed for job {job_id}"

# DAG ì •ì˜
default_args = {
    "owner": "data-team",
    "depends_on_past": False,
    "start_date": datetime(2025, 1, 1),
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5)
}

with DAG(
    dag_id="summary_analysis_dag",
    default_args=default_args,
    description="ì „ì²´ ë¦¬ë·° ìš”ì•½ ë¶„ì„ ìš”ì²­ ë° ì™„ë£Œ ê°ì§€",
    schedule=None,
    catchup=False,
    tags=["summary", "analysis", "kafka", "llm"]
) as dag:

    # 1. Overall Summary Request Topicì— ë©”ì‹œì§€ ë°œí–‰
    publish_summary_request_task = PythonOperator(
        task_id="publish_summary_request",
        python_callable=publish_summary_request,
    )

    # 2. Control Topicì—ì„œ ìš”ì•½ ì™„ë£Œ ë©”ì‹œì§€ ì„¼ì‹± ëŒ€ê¸°
    wait_summary_completion = AwaitMessageSensor(
        task_id="wait_summary_completion",
        kafka_config_id="job-control-topic",
        topics=["job-control-topic"],
        apply_function="include.kafka_filters.control_message_check",
        apply_function_args=[
            "{{ dag_run.conf.get('job_id') if dag_run and dag_run.conf else run_id }}",
            "summary"
        ],
        poll_timeout=1,
        poll_interval=30,  # 30ì´ˆë§ˆë‹¤ ì²´í¬
        execution_timeout=timedelta(minutes=30),  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
        xcom_push_key="summary_completion_message",
        retries=0,
        on_failure_callback=handle_summary_failure
    )

    # 3. ì™„ë£Œ ì•Œë¦¼
    notify_completion = PythonOperator(
        task_id="notify_summary_completion",
        python_callable=notify_summary_completion,
    )

    # ì‘ì—… ìˆœì„œ ì •ì˜
    publish_summary_request_task >> wait_summary_completion >> notify_completion

"""
DAG ì‹¤í–‰ ë°©ë²•:

1. Redshift COPY ì™„ë£Œ í›„ ìë™ íŠ¸ë¦¬ê±°:
   airflow dags trigger summary_analysis_dag --conf '{
     "job_id": "job-2024-001",
     "execution_time": "2024-01-15T13:30:00+09:00",
     "copy_completion_time": "2024-01-15T14:45:00+09:00",
     "redshift_copy_completed": true,
     "source_dag": "redshift_s3_copy_pipeline"
   }'

2. ìˆ˜ë™ ì‹¤í–‰:
   airflow dags trigger summary_analysis_dag --conf '{
     "job_id": "job-2024-001",
     "execution_time": "2024-01-15T13:30:00+09:00"
   }'

3. Airflow UIì—ì„œ ìˆ˜ë™ ì‹¤í–‰:
   - DAG í˜ì´ì§€ì—ì„œ "Trigger DAG w/ Config" í´ë¦­
   - Configuration JSONì— ìœ„ ì˜ˆì‹œ ì…ë ¥

ì£¼ì˜ì‚¬í•­:
- job_idëŠ” Control í† í”½ì˜ ë©”ì‹œì§€ì™€ ì¼ì¹˜í•´ì•¼ í•¨
- execution_timeì€ realtime_pipeline_monitor_dagì˜ ì‹¤í–‰ ì‹œê°„
- Overall Summary Request Topic ë©”ì‹œì§€ ë°œí–‰ í›„ Control Topicì—ì„œ ìš”ì•½ ì™„ë£Œ ë©”ì‹œì§€ ëŒ€ê¸°
- íƒ€ì„ì•„ì›ƒ: 30ë¶„ (LLM ë¶„ì„ ì‹œê°„ ê³ ë ¤)
- ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ì—ëŸ¬ ë¡œê¹… ë° ì•Œë¦¼ ì²˜ë¦¬
- Kafka ì—°ê²° IDëŠ” 'job-control-topic'ìœ¼ë¡œ ì„¤ì • í•„ìš”
- KAFKA_BOOTSTRAP_SERVERS í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”

ë©”ì‹œì§€ êµ¬ì¡°:
- Overall Summary Request Topic: job_id, execution_time, copy_completion_time í¬í•¨
- Control Topic: summary ë‹¨ê³„ ì™„ë£Œ ë©”ì‹œì§€ ì„¼ì‹±

Kafka Producer ì„¤ì •:
- í† í”½: overall-summary-request-topic
- ì§ë ¬í™”: JSON (UTF-8 ì¸ì½”ë”©)
- ì¬ì‹œë„: 3íšŒ
- ACK: all (ëª¨ë“  ë³µì œë³¸ í™•ì¸)
- íƒ€ì„ì•„ì›ƒ: 30ì´ˆ (ìš”ì²­), 120ì´ˆ (ì „ì†¡)
"""
