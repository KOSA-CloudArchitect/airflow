from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.apache.kafka.sensors.kafka import AwaitMessageSensor
from airflow.providers.apache.kafka.operators.produce import ProduceToTopicOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.models.xcom_arg import XComArg

from datetime import datetime, timedelta
import logging
import json
import pytz
import os

def kafka_producer_function(job_id, **context):
    """Kafka Producer í•¨ìˆ˜ - ProduceToTopicOperatorì—ì„œ ì‚¬ìš©"""
    logging.info(f"[Kafka Producer] Received job_id: {job_id}")
    
    # XComì—ì„œ ì „ì²´ summary_message ê°€ì ¸ì˜¤ê¸° ì‹œë„
    summary_message = None
    
    # ë°©ë²• 1: task_instanceë¥¼ í†µí•œ XCom ì ‘ê·¼
    try:
        task_instance = context.get('task_instance')
        if task_instance:
            summary_message = task_instance.xcom_pull(
                task_ids='prepare_summary_message',
                key='summary_request_message'
            )
            logging.info(f"[Kafka Producer] Retrieved XCom data: {summary_message}")
    except Exception as e:
        logging.warning(f"[Kafka Producer] XCom access failed: {e}")
    
    # XCom ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ job_idë¡œ ê°„ë‹¨í•œ ë©”ì‹œì§€ ìƒì„±
    if summary_message:
        logging.info(f"[Kafka Producer] Using XCom data for job_id: {job_id}")
        message_data = summary_message
    else:
        logging.warning(f"[Kafka Producer] Using fallback message for job_id: {job_id}")
        message_data = {
            "job_id": job_id,
            "timestamp": datetime.now().isoformat(),
            "message": f"Publishing summary request for {job_id}",
            "warning": "XCom data not available"
        }
    
    # JSON ì§ë ¬í™”
    try:
        json_str = json.dumps(message_data, ensure_ascii=False)
        logging.info(f"[Kafka Producer] JSON serialization successful: {json_str}")
    except Exception as e:
        logging.error(f"[Kafka Producer] JSON serialization failed: {e}")
        # ì•ˆì „í•œ fallback
        fallback_data = {
            "job_id": str(job_id),
            "timestamp": datetime.now().isoformat(),
            "error": f"JSON serialization failed: {str(e)}"
        }
        json_str = json.dumps(fallback_data, ensure_ascii=False)
    
    return [{
        "key": job_id.encode("utf-8"),
        "value": json_str.encode("utf-8")
    }]

def prepare_summary_request_message(**context):
    """Overall Summary Request ë©”ì‹œì§€ ì¤€ë¹„"""
    dag_run = context.get("dag_run")
    conf = dag_run.conf or {}
    
    # Redshift COPY DAGì—ì„œ ì „ë‹¬ë°›ì€ ë°ì´í„° ì¶”ì¶œ
    job_id = conf.get('job_id', 'unknown')
    execution_time = conf.get('execution_time', '')
    copy_completion_time = conf.get('copy_completion_time', '')
    source_dag = conf.get('source_dag', 'unknown')
    
    # í•œêµ­ ì‹œê°„ìœ¼ë¡œ í˜„ì¬ ì‹œê°„ ê³„ì‚°
    kst = pytz.timezone('Asia/Seoul')
    current_time_kst = datetime.now(kst)
    
    # Overall Summary Request ë©”ì‹œì§€ êµ¬ì„±
    summary_request_message = {
        "job_id": job_id,
        "request_type": "overall_summary",
        "execution_time": execution_time,  # realtime_pipeline_monitor_dag ì‹¤í–‰ ì‹œê°„
        "copy_completion_time": copy_completion_time,  # Redshift COPY ì™„ë£Œ ì‹œê°„
        "request_timestamp": current_time_kst.isoformat(),
        "metadata": {
            "source_dag": source_dag,
            "trigger_point": "redshift_copy_completed",
            "analysis_type": "llm_summary",
            "worker_id": "airflow-summary-worker",
            "pipeline_execution_time": execution_time  # ì›ë³¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œê°„
        }
    }
    
    logging.info(f"[Summary Request] Preparing message for job_id={job_id}")
    logging.info(f"[Summary Request] Execution time from pipeline: {execution_time}")
    logging.info(f"[Summary Request] Copy completion time: {copy_completion_time}")
    logging.info(f"[Summary Request] Message: {summary_request_message}")
    
    # XComì— ë©”ì‹œì§€ ì €ì¥
    context['task_instance'].xcom_push(key='summary_request_message', value=summary_request_message)
    
    return summary_request_message

def handle_summary_failure(context):
    """ìš”ì•½ ë¶„ì„ ì‹¤íŒ¨ ì²˜ë¦¬ í•¨ìˆ˜"""
    dag_run = context.get("dag_run")
    job_id = (dag_run.conf or {}).get("job_id") if dag_run else None
    task_id = context.get("task_instance").task_id
    
    logging.error(f"Summary analysis failed for job {job_id} at task {task_id}")
    print(f"ğŸš¨ Summary analysis failure: Job {job_id} failed at {task_id}", flush=True)
    
    # Discord/Slack ì•Œë¦¼ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
    # send_summary_failure_alert(context=context, job_id=job_id, task_id=task_id)

def notify_summary_completion(**context):
    """ìš”ì•½ ë¶„ì„ ì™„ë£Œ ì•Œë¦¼"""
    dag_run = context.get("dag_run")
    job_id = (dag_run.conf or {}).get("job_id") if dag_run else None
    
    logging.info(f"Summary analysis completed successfully for job {job_id}")
    print(f"ğŸ‰ Summary analysis completed for job {job_id}!", flush=True)
    
    return f"Summary analysis completed for job {job_id}"

# DAG ì •ì˜
default_args = {
    "owner": "data-team",
    "depends_on_past": False,
    "start_date": datetime(2025, 1, 1),
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5)
}

with DAG(
    dag_id="summary_analysis_dag",
    default_args=default_args,
    description="ì „ì²´ ë¦¬ë·° ìš”ì•½ ë¶„ì„ ìš”ì²­ ë° ì™„ë£Œ ê°ì§€",
    schedule=None,
    catchup=False,
    tags=["summary", "analysis", "kafka", "llm"]
) as dag:

    # 1. Overall Summary Request ë©”ì‹œì§€ ì¤€ë¹„
    prepare_summary_message = PythonOperator(
        task_id="prepare_summary_message",
        python_callable=prepare_summary_request_message,
    )

    # 2. Overall Summary Request Topicì— ë©”ì‹œì§€ ë°œí–‰
    publish_summary_request = ProduceToTopicOperator(
        task_id="publish_summary_request",
        topic="overall-summary-request-topic",
        kafka_config_id="overall-summary-request-topic",
        producer_function=kafka_producer_function,
        op_kwargs={
            "job_id": "{{ ti.xcom_pull(task_ids='prepare_summary_message', key='summary_request_message')['job_id'] }}"
        },
    )

    # 3. Control Topicì—ì„œ ìš”ì•½ ì™„ë£Œ ë©”ì‹œì§€ ì„¼ì‹± ëŒ€ê¸°
    wait_summary_completion = AwaitMessageSensor(
        task_id="wait_summary_completion",
        kafka_config_id="job-control-topic-overall",
        topics=["job-control-topic"],
        apply_function="include.kafka_filters.control_message_check",
        apply_function_args=[
            "{{ dag_run.conf.get('job_id') if dag_run and dag_run.conf else run_id }}",
            "summary"
        ],
        poll_timeout=1,
        poll_interval=30,  # 30ì´ˆë§ˆë‹¤ ì²´í¬
        execution_timeout=timedelta(minutes=30),  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
        xcom_push_key="summary_completion_message",
        retries=0,
        on_failure_callback=handle_summary_failure
    )

    # 4. ì™„ë£Œ ì•Œë¦¼
    notify_completion = PythonOperator(
        task_id="notify_summary_completion",
        python_callable=notify_summary_completion,
    )

    # ì‘ì—… ìˆœì„œ ì •ì˜
    prepare_summary_message >> publish_summary_request >> wait_summary_completion >> notify_completion

"""
DAG ì‹¤í–‰ ë°©ë²•:

1. Redshift COPY ì™„ë£Œ í›„ ìë™ íŠ¸ë¦¬ê±°:
   airflow dags trigger summary_analysis_dag --conf '{
     "job_id": "job-2024-001",
     "execution_time": "2024-01-15T13:30:00+09:00",
     "copy_completion_time": "2024-01-15T14:45:00+09:00",
     "redshift_copy_completed": true,
     "source_dag": "redshift_s3_copy_pipeline"
   }'

2. ìˆ˜ë™ ì‹¤í–‰:
   airflow dags trigger summary_analysis_dag --conf '{
     "job_id": "job-2024-001",
     "execution_time": "2024-01-15T13:30:00+09:00"
   }'

3. Airflow UIì—ì„œ ìˆ˜ë™ ì‹¤í–‰:
   - DAG í˜ì´ì§€ì—ì„œ "Trigger DAG w/ Config" í´ë¦­
   - Configuration JSONì— ìœ„ ì˜ˆì‹œ ì…ë ¥

ì£¼ì˜ì‚¬í•­:
- job_idëŠ” Control í† í”½ì˜ ë©”ì‹œì§€ì™€ ì¼ì¹˜í•´ì•¼ í•¨
- execution_timeì€ realtime_pipeline_monitor_dagì˜ ì‹¤í–‰ ì‹œê°„
- Overall Summary Request Topic ë©”ì‹œì§€ ë°œí–‰ í›„ Control Topicì—ì„œ ìš”ì•½ ì™„ë£Œ ë©”ì‹œì§€ ëŒ€ê¸°
- íƒ€ì„ì•„ì›ƒ: 30ë¶„ (LLM ë¶„ì„ ì‹œê°„ ê³ ë ¤)
- ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ì—ëŸ¬ ë¡œê¹… ë° ì•Œë¦¼ ì²˜ë¦¬
- Kafka ì—°ê²° IDëŠ” 'job-control-topic'ìœ¼ë¡œ ì„¤ì • í•„ìš”
- KAFKA_BOOTSTRAP_SERVERS í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”

ë©”ì‹œì§€ êµ¬ì¡°:
- Overall Summary Request Topic: job_id, execution_time, copy_completion_time í¬í•¨
- Control Topic: summary ë‹¨ê³„ ì™„ë£Œ ë©”ì‹œì§€ ì„¼ì‹±

Airflow 3.0 Kafka Producer ì„¤ì •:
- í† í”½: overall-summary-request-topic
- ì§ë ¬í™”: JSON (UTF-8 ì¸ì½”ë”©)
- Producer Operator ì‚¬ìš©ìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ
- í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ Kafka ì„œë²„ ì„¤ì •
"""
