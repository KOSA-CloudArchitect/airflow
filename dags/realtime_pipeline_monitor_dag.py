from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.http.operators.http import HttpOperator
from airflow.providers.apache.kafka.sensors.kafka import AwaitMessageSensor
from datetime import datetime, timedelta
import logging
import json

def log_request(**context):
    """DAG íŠ¸ë¦¬ê±° ì‹œ ë°›ì€ ì„¤ì • ì •ë³´ ë¡œê¹…"""
    conf = context['dag_run'].conf or {}
    logging.info(f"[Pipeline Monitor DAG Triggered] Received conf: {conf}")

def log_crawler_callback(context):
    """í¬ë¡¤ëŸ¬ í˜¸ì¶œ ì‹œ job_id ë¡œê¹…"""
    dag_run = context.get("dag_run")
    job_id = (dag_run.conf or {}).get("job_id") if dag_run else None
    logging.info(f"[call_crawler] job_id={job_id}")

def determine_crawler_type(conf):
    """í¬ë¡¤ë§ íƒ€ì…ì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        conf: DAG ì‹¤í–‰ ì‹œ ì „ë‹¬ë°›ì€ ì„¤ì • ë°ì´í„°
    
    Returns:
        str: 'multi' ë˜ëŠ” 'single'
    """
    # url_listê°€ ìˆìœ¼ë©´ ë‹¤ì¤‘ ìƒí’ˆ í¬ë¡¤ë§
    if 'url_list' in conf and isinstance(conf['url_list'], list):
        return 'multi'
    
    # urlì´ ìˆìœ¼ë©´ ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§
    if 'url' in conf:
        return 'single'
    
    # ê¸°ë³¸ê°’ì€ ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§
    return 'single'

def build_crawler_request_payload(conf):
    """í¬ë¡¤ë§ íƒ€ì…ì— ë”°ë¼ ìš”ì²­ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        conf: DAG ì‹¤í–‰ ì‹œ ì „ë‹¬ë°›ì€ ì„¤ì • ë°ì´í„°
    
    Returns:
        tuple: (endpoint, request_data)
    """
    job_id = conf.get('job_id')
    crawler_type = determine_crawler_type(conf)
    
    if crawler_type == 'multi':
        # ë‹¤ì¤‘ ìƒí’ˆ í¬ë¡¤ë§
        url_list = conf.get('url_list', [])
        if not url_list:
            raise ValueError("ë‹¤ì¤‘ ìƒí’ˆ í¬ë¡¤ë§ì„ ìœ„í•´ì„œëŠ” url_listê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        endpoint = "/crawl/product_multi"
        request_data = {
            "url_list": url_list,
            "job_id": job_id
        }
        
        logging.info(f"[Multi Crawler] endpoint={endpoint}, url_count={len(url_list)}")
        
    else:
        # ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§
        product_id = conf.get('product_id')
        url = conf.get('url')
        review_cnt = conf.get('review_cnt', 0)
        
        if not product_id or not url:
            raise ValueError("ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§ì„ ìœ„í•´ì„œëŠ” product_idì™€ urlì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        endpoint = "/crawl/product_one"
        request_data = {
            "product_id": product_id,
            "url": url,
            "job_id": job_id,
            "review_cnt": review_cnt
        }
        
        logging.info(f"[Single Crawler] endpoint={endpoint}, product_id={product_id}")
    
    return endpoint, request_data

def call_crawler_dynamic(**context):
    """ë™ì ìœ¼ë¡œ í¬ë¡¤ëŸ¬ APIë¥¼ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    conf = context['dag_run'].conf or {}
    
    try:
        endpoint, request_data = build_crawler_request_payload(conf)
        
        logging.info(f"[Dynamic Crawler] Calling endpoint: {endpoint}")
        logging.info(f"[Dynamic Crawler] Request data: {request_data}")
        
        # ì‹¤ì œ HTTP ìš”ì²­ì€ HttpOperatorì—ì„œ ì²˜ë¦¬ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë°ì´í„°ë§Œ ì¤€ë¹„
        return {
            'endpoint': endpoint,
            'data': request_data
        }
        
    except Exception as e:
        logging.error(f"[Dynamic Crawler] Error preparing request: {e}")
        raise

# Control í† í”½ ë©”ì‹œì§€ í•„í„°ë§ í•¨ìˆ˜ëŠ” include/kafka_filters.pyì—ì„œ import

def handle_step_failure(context):
    """ë‹¨ê³„ë³„ ì‹¤íŒ¨ ì²˜ë¦¬ í•¨ìˆ˜"""
    dag_run = context.get("dag_run")
    job_id = (dag_run.conf or {}).get("job_id") if dag_run else None
    task_id = context.get("task_instance").task_id
    
    # task_idì—ì„œ ë‹¨ê³„ ì¶”ì¶œ (wait_collection -> collection)
    step = task_id.replace("wait_", "")
    
    logging.error(f"Job {job_id} failed at {step} step")
    print(f"ğŸš¨ Pipeline failure: Job {job_id} failed at {step} step", flush=True)
    
    # ì¶”ê°€ ì•Œë¦¼ ë¡œì§ êµ¬í˜„ ê°€ëŠ¥ (Slack, ì´ë©”ì¼ ë“±)

# DAG ì •ì˜
default_args = {
    "owner": "data-team",
    "depends_on_past": False,
    "start_date": datetime(2025, 1, 1),
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5)
}

with DAG(
    dag_id="realtime_pipeline_monitor",
    default_args=default_args,
    description="ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ (Control í† í”½ ê¸°ë°˜)",
    schedule=None,
    catchup=False,
    tags=["pipeline", "monitor", "control-topic", "realtime"]
) as dag:

    # 1. ìš”ì²­ ë¡œê¹…
    log_request_task = PythonOperator(
        task_id="log_request",
        python_callable=log_request,
    )

    # 2. í¬ë¡¤ë§ íƒ€ì… ê²°ì • ë° ìš”ì²­ ë°ì´í„° ì¤€ë¹„
    prepare_crawler_request_task = PythonOperator(
        task_id="prepare_crawler_request",
        python_callable=call_crawler_dynamic,
    )

    # 3. Crawler ì„œë²„ì— ë™ì  HTTP ìš”ì²­
    call_crawler = HttpOperator(
        task_id="call_crawler",
        http_conn_id="crawler_server",  # ì‹¤ì œ í¬ë¡¤ëŸ¬ ì„œë²„ ì—°ê²° ID
        endpoint="{{ ti.xcom_pull(task_ids='prepare_crawler_request')['endpoint'] }}",
        method="POST",
        data="{{ ti.xcom_pull(task_ids='prepare_crawler_request')['data'] | tojson }}",
        headers={"Content-Type": "application/json"},
        on_execute_callback=log_crawler_callback,
        log_response=True,
    )

    # 3. Collection ë‹¨ê³„ ì™„ë£Œ ëŒ€ê¸°
    wait_collection = AwaitMessageSensor(
        task_id="wait_collection",
        kafka_config_id="job-control-topic",  # Control í† í”½ ì—°ê²° ID
        topics=["job-control-topic"],
        apply_function="include.kafka_filters.control_message_check",
        apply_function_args=[
            "{{ dag_run.conf.get('job_id') if dag_run and dag_run.conf else run_id }}",
            "collection"
        ],
        poll_timeout=1,
        poll_interval=30,  # 30ì´ˆë§ˆë‹¤ ì²´í¬
        execution_timeout=timedelta(minutes=60),  # 60ë¶„ íƒ€ì„ì•„ì›ƒ
        xcom_push_key="collection_message",
        retries=0,
        on_failure_callback=handle_step_failure
    )

    # 4. Transform ë‹¨ê³„ ì™„ë£Œ ëŒ€ê¸°
    wait_transform = AwaitMessageSensor(
        task_id="wait_transform",
        kafka_config_id="job-control-topic",
        topics=["job-control-topic"],
        apply_function="include.kafka_filters.control_message_check",
        apply_function_args=[
            "{{ dag_run.conf.get('job_id') if dag_run and dag_run.conf else run_id }}",
            "transform"
        ],
        poll_timeout=1,
        poll_interval=30,
        execution_timeout=timedelta(minutes=30),  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
        xcom_push_key="transform_message",
        retries=0,
        on_failure_callback=handle_step_failure
    )

    # 5. Analysis ë‹¨ê³„ ì™„ë£Œ ëŒ€ê¸°
    wait_analysis = AwaitMessageSensor(
        task_id="wait_analysis",
        kafka_config_id="job-control-topic",
        topics=["job-control-topic"],
        apply_function="include.kafka_filters.control_message_check",
        apply_function_args=[
            "{{ dag_run.conf.get('job_id') if dag_run and dag_run.conf else run_id }}",
            "analysis"
        ],
        poll_timeout=1,
        poll_interval=30,
        execution_timeout=timedelta(minutes=45),  # 45ë¶„ íƒ€ì„ì•„ì›ƒ
        xcom_push_key="analysis_message",
        retries=0,
        on_failure_callback=handle_step_failure
    )

    # 6. Aggregation ë‹¨ê³„ ì™„ë£Œ ëŒ€ê¸°
    wait_aggregation = AwaitMessageSensor(
        task_id="wait_aggregation",
        kafka_config_id="job-control-topic",
        topics=["job-control-topic"],
        apply_function="include.kafka_filters.control_message_check",
        apply_function_args=[
            "{{ dag_run.conf.get('job_id') if dag_run and dag_run.conf else run_id }}",
            "aggregation"
        ],
        poll_timeout=1,
        poll_interval=30,
        execution_timeout=timedelta(minutes=15),  # 15ë¶„ íƒ€ì„ì•„ì›ƒ
        xcom_push_key="aggregation_message",
        retries=0,
        on_failure_callback=handle_step_failure
    )

    # 7. ì™„ë£Œ ì•Œë¦¼
    notify_completion = PythonOperator(
        task_id="notify_completion",
        python_callable=lambda: print("ğŸ‰ All pipeline steps completed successfully!", flush=True)
    )

    # ì‘ì—… ìˆœì„œ ì •ì˜
    log_request_task >> prepare_crawler_request_task >> call_crawler >> wait_collection >> wait_transform >> wait_analysis >> wait_aggregation >> notify_completion

"""
DAG ì‹¤í–‰ ë°©ë²•:

1. ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§ (ê¸°ë³¸):
   airflow dags trigger realtime_pipeline_monitor --conf '{
     "job_id": "job-2024-001",
     "product_id": "product-123",
     "url": "https://example.com/product/123",
     "review_cnt": 100
   }'

2. ë‹¤ì¤‘ ìƒí’ˆ í¬ë¡¤ë§:
   airflow dags trigger realtime_pipeline_monitor --conf '{
     "job_id": "job-2024-002",
     "url_list": [
       "https://example.com/product/123",
       "https://example.com/product/456",
       "https://example.com/product/789"
     ]
   }'

3. Airflow UIì—ì„œ ìˆ˜ë™ ì‹¤í–‰:
   - DAG í˜ì´ì§€ì—ì„œ "Trigger DAG w/ Config" í´ë¦­
   - Configuration JSONì— ìœ„ ì˜ˆì‹œ ì¤‘ í•˜ë‚˜ ì…ë ¥

4. ì™¸ë¶€ APIë¡œ ì‹¤í–‰:
   curl -X POST "http://airflow-server:8080/api/v1/dags/realtime_pipeline_monitor/dagRuns" \
        -H "Content-Type: application/json" \
        -d '{
          "conf": {
            "job_id": "job-2024-001",
            "product_id": "product-123",
            "url": "https://example.com/product/123",
            "review_cnt": 100
          }
        }'

API ì—”ë“œí¬ì¸íŠ¸ ìë™ ì„ íƒ:
- url_listê°€ ìˆìœ¼ë©´ â†’ /crawl/product_multi (ë‹¤ì¤‘ ìƒí’ˆ í¬ë¡¤ë§)
- urlì´ ìˆìœ¼ë©´ â†’ /crawl/product_one (ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§)

ì£¼ì˜ì‚¬í•­:
- job_idëŠ” Control í† í”½ì˜ ë©”ì‹œì§€ì™€ ì¼ì¹˜í•´ì•¼ í•¨
- ê° ë‹¨ê³„ë³„ íƒ€ì„ì•„ì›ƒ: Collection(60ë¶„), Transform(30ë¶„), Analysis(45ë¶„), Aggregation(15ë¶„)
- ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ì—ëŸ¬ ë¡œê¹… ë° ì•Œë¦¼ ì²˜ë¦¬
- í¬ë¡¤ëŸ¬ ì„œë²„ ì—°ê²° IDëŠ” 'crawler_server'ë¡œ ì„¤ì • í•„ìš”
"""
