from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.http.operators.http import HttpOperator
from airflow.providers.apache.kafka.sensors.kafka import AwaitMessageSensor
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.models.xcom_arg import XComArg
 
from datetime import datetime, timedelta
import logging
import json
import pytz

# Discord ì•Œë¦¼ ìœ í‹¸ì€ dags/include/discord_notifier.pyì— êµ¬í˜„
# ì¤€ë¹„ë˜ë©´ ì•„ë˜ importì˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
# from include.discord_notifier import send_discord_failure_alert

def log_crawler_callback(context):
    """í¬ë¡¤ëŸ¬ í˜¸ì¶œ ì‹œ job_id ë¡œê¹… ë° ì‹¤í–‰ ì‹œê°„ ì €ì¥"""
    dag_run = context.get("dag_run")
    job_id = (dag_run.conf or {}).get("job_id") if dag_run else None
    
    # í•œêµ­ ì‹œê°„ìœ¼ë¡œ í˜„ì¬ ì‹œê°„ ê³„ì‚°
    kst = pytz.timezone('Asia/Seoul')
    current_time_kst = datetime.now(kst)
    
    logging.info(f"[call_crawler] job_id={job_id}, execution_time={current_time_kst.isoformat()}")
    
    # XComì— ì‹¤í–‰ ì‹œê°„ ì €ì¥
    context['task_instance'].xcom_push(key='crawler_execution_time', value=current_time_kst.isoformat())

 

def determine_crawler_type(conf):
    """í¬ë¡¤ë§ íƒ€ì…ì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        conf: DAG ì‹¤í–‰ ì‹œ ì „ë‹¬ë°›ì€ ì„¤ì • ë°ì´í„°
    
    Returns:
        str: 'multi' ë˜ëŠ” 'single'
    """
    # url_listê°€ ìˆìœ¼ë©´ ë‹¤ì¤‘ ìƒí’ˆ í¬ë¡¤ë§
    if 'url_list' in conf and isinstance(conf['url_list'], list):
        return 'multi'
    
    # urlì´ ìˆìœ¼ë©´ ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§
    if 'url' in conf:
        return 'single'
    
    # ê¸°ë³¸ê°’ì€ ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§
    return 'single'

def build_crawler_request_payload(conf):
    """í¬ë¡¤ë§ íƒ€ì…ì— ë”°ë¼ ìš”ì²­ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        conf: DAG ì‹¤í–‰ ì‹œ ì „ë‹¬ë°›ì€ ì„¤ì • ë°ì´í„°
    
    Returns:
        tuple: (endpoint, request_data)
    """
    job_id = conf.get('job_id')
    crawler_type = determine_crawler_type(conf)
    
    if crawler_type == 'multi':
        # ë‹¤ì¤‘ ìƒí’ˆ í¬ë¡¤ë§
        url_list = conf.get('url_list', [])
        if not url_list:
            raise ValueError("ë‹¤ì¤‘ ìƒí’ˆ í¬ë¡¤ë§ì„ ìœ„í•´ì„œëŠ” url_listê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        endpoint = "/crawl/product_multi"
        request_data = {
            "url_list": url_list,
            "job_id": job_id
        }
        
        logging.info(f"[Multi Crawler] endpoint={endpoint}, url_count={len(url_list)}")
        
    else:
        # ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§
        product_id = conf.get('product_id')  # ì„ íƒì  í•„ë“œ
        url = conf.get('url')
        review_cnt = conf.get('review_cnt', 0)
        
        if not url:
            raise ValueError("ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§ì„ ìœ„í•´ì„œëŠ” urlì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        endpoint = "/crawl/product_one"
        request_data = {
            "url": url,
            "job_id": job_id,
            "review_cnt": review_cnt
        }
        
        # product_idê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if product_id:
            request_data["product_id"] = product_id
        
        logging.info(f"[Single Crawler] endpoint={endpoint}, url={url}, product_id={product_id or 'None'}")
    
    return endpoint, request_data

def call_crawler_dynamic(**context):
    """ë™ì ìœ¼ë¡œ í¬ë¡¤ëŸ¬ APIë¥¼ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    import requests
    from airflow.hooks.base import BaseHook
    
    conf = context['dag_run'].conf or {}
    
    try:
        endpoint, request_data = build_crawler_request_payload(conf)
        
        logging.info(f"[Dynamic Crawler] Calling endpoint: {endpoint}")
        logging.info(f"[Dynamic Crawler] Request data: {request_data}")
        
        # í¬ë¡¤ëŸ¬ ì„œë²„ ì—°ê²° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        crawler_conn = BaseHook.get_connection("crawler_server")
        base_url = crawler_conn.host
        if crawler_conn.port:
            base_url = f"{base_url}:{crawler_conn.port}"
        
        # ì‹¤ì œ HTTP ìš”ì²­ ìˆ˜í–‰
        url = f"{base_url}{endpoint}"
        headers = {"Content-Type": "application/json"}
        
        response = requests.post(url, json=request_data, headers=headers, timeout=30)
        response.raise_for_status()
        
        # ì‹¤í–‰ ì‹œê°„ ì €ì¥ (KST)
        kst = pytz.timezone('Asia/Seoul')
        execution_time_kst = datetime.now(kst)
        
        # XComì— ì‹¤í–‰ ì‹œê°„ ì €ì¥
        context['task_instance'].xcom_push(
            key='crawler_execution_time',
            value=execution_time_kst.isoformat()
        )
        
        logging.info(f"[Dynamic Crawler] Response: {response.status_code}")
        print(f"âœ… Crawler request completed successfully", flush=True)
        
        return {
            'status': 'success',
            'response_code': response.status_code,
            'execution_time': execution_time_kst.isoformat()
        }
        
    except Exception as e:
        logging.error(f"[Dynamic Crawler] Error calling crawler: {e}")
        print(f"âŒ Crawler request failed: {e}", flush=True)
        raise

# Control í† í”½ ë©”ì‹œì§€ í•„í„°ë§ í•¨ìˆ˜ëŠ” include/kafka_filters.pyì—ì„œ import

def handle_step_failure(context):
    """ë‹¨ê³„ë³„ ì‹¤íŒ¨ ì²˜ë¦¬ í•¨ìˆ˜"""
    dag_run = context.get("dag_run")
    job_id = (dag_run.conf or {}).get("job_id") if dag_run else None
    task_id = context.get("task_instance").task_id
    
    # task_idì—ì„œ ë‹¨ê³„ ì¶”ì¶œ (wait_collection -> collection)
    step = task_id.replace("wait_", "")
    
    logging.error(f"Job {job_id} failed at {step} step")
    print(f"ğŸš¨ Pipeline failure: Job {job_id} failed at {step} step", flush=True)
    
    # Discordë¡œ ì‹¤íŒ¨ ì•Œë¦¼ ì „ì†¡ (ì‹¤ì‚¬ìš© ì „ê¹Œì§€ ì£¼ì„ ìœ ì§€)
    # try:
    #     send_discord_failure_alert(context=context, job_id=job_id, step=step)
    # except Exception as e:
    #     logging.error(f"[Discord Notify] Failed to send alert: {e}")

def prepare_redshift_trigger_data(**context):
    """Redshift DAG íŠ¸ë¦¬ê±°ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
    dag_run = context.get("dag_run")
    job_id = (dag_run.conf or {}).get("job_id") if dag_run else None
    
    # call_crawlerì—ì„œ ì €ì¥í•œ ì‹¤í–‰ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
    crawler_execution_time = context['task_instance'].xcom_pull(
        task_ids='call_crawler', 
        key='crawler_execution_time'
    )
    
    if crawler_execution_time:
        execution_time_kst = datetime.fromisoformat(crawler_execution_time)
    else:
        # fallback: í˜„ì¬ ì‹œê°„ ì‚¬ìš©
        kst = pytz.timezone('Asia/Seoul')
        execution_time_kst = datetime.now(kst)
    
    trigger_data = {
        'job_id': job_id,
        'execution_time': execution_time_kst.isoformat(),
        'dag_run_id': context['dag_run'].run_id,
        'source_dag': 'realtime_pipeline_monitor',
        'trigger_point': 'call_crawler'
    }
    
    logging.info(f"[Redshift Trigger] Prepared data: {trigger_data}")
    print(f"ğŸ”„ Triggering Redshift DAG with job_id={job_id}, execution_time={execution_time_kst.isoformat()} (call_crawler execution time)", flush=True)
    
    return trigger_data

# DAG ì •ì˜
default_args = {
    "owner": "data-team",
    "depends_on_past": False,
    "start_date": datetime(2025, 1, 1),
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5)
}

with DAG(
    dag_id="realtime_pipeline_monitor",
    default_args=default_args,
    description="ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§ (Control í† í”½ ê¸°ë°˜)",
    schedule=None,
    catchup=False,
    tags=["pipeline", "monitor", "control-topic", "realtime"]
) as dag:

    # 1. Crawler ì„œë²„ì— ë™ì  HTTP ìš”ì²­ (ë°ì´í„° ì¤€ë¹„ í¬í•¨)
    call_crawler = PythonOperator(
        task_id="call_crawler",
        python_callable=lambda **context: (
            print("ğŸš€ Starting crawler request...", flush=True),
            call_crawler_dynamic(**context)
        )[1],  # call_crawler_dynamicì˜ ê²°ê³¼ ë°˜í™˜
    )

    # 3. Collection ë‹¨ê³„ ì™„ë£Œ ëŒ€ê¸°
    wait_collection = AwaitMessageSensor(
        task_id="wait_collection",
        kafka_config_id="job-control-topic",  # Control í† í”½ ì—°ê²° ID
        topics=["job-control-topic"],
        apply_function="include.kafka_filters.control_message_check",
        apply_function_args=[
            "{{ dag_run.conf.get('job_id') if dag_run and dag_run.conf else run_id }}",
            "collection"
        ],
        poll_timeout=1,
        poll_interval=10,  # 30ì´ˆë§ˆë‹¤ ì²´í¬
        execution_timeout=timedelta(minutes=3),
        xcom_push_key="collection_message",
        retries=0,
        on_failure_callback=handle_step_failure
    )

    # 4. Transform ë‹¨ê³„ ì™„ë£Œ ëŒ€ê¸° (ë³‘ë ¬ ì²˜ë¦¬)
    wait_transform = AwaitMessageSensor(
        task_id="wait_transform",
        kafka_config_id="job-control-topic",
        topics=["job-control-topic"],
        apply_function="include.kafka_filters.control_message_check",
        apply_function_args=[
            "{{ dag_run.conf.get('job_id') if dag_run and dag_run.conf else run_id }}",
            "transform"
        ],
        poll_timeout=1,
        poll_interval=10,
        execution_timeout=timedelta(minutes=3),
        xcom_push_key="transform_message",
        retries=0,
        on_failure_callback=handle_step_failure
    )

    # 5. Analysis ë‹¨ê³„ ì™„ë£Œ ëŒ€ê¸° (ë³‘ë ¬ ì²˜ë¦¬)
    wait_analysis = AwaitMessageSensor(
        task_id="wait_analysis",
        kafka_config_id="job-control-topic",
        topics=["job-control-topic"],
        apply_function="include.kafka_filters.control_message_check",
        apply_function_args=[
            "{{ dag_run.conf.get('job_id') if dag_run and dag_run.conf else run_id }}",
            "analysis"
        ],
        poll_timeout=1,
        poll_interval=10,
        execution_timeout=timedelta(minutes=5),
        xcom_push_key="analysis_message",
        retries=0,
        on_failure_callback=handle_step_failure
    )

    # 6. Aggregation ë‹¨ê³„ ì™„ë£Œ ëŒ€ê¸°
    wait_aggregation = AwaitMessageSensor(
        task_id="wait_aggregation",
        kafka_config_id="job-control-topic",
        topics=["job-control-topic"],
        apply_function="include.kafka_filters.control_message_check",
        apply_function_args=[
            "{{ dag_run.conf.get('job_id') if dag_run and dag_run.conf else run_id }}",
            "aggregation"
        ],
        poll_timeout=1,
        poll_interval=10,
        execution_timeout=timedelta(minutes=5),
        xcom_push_key="aggregation_message",
        retries=0,
        on_failure_callback=handle_step_failure
    )

    # 7. Redshift DAG íŠ¸ë¦¬ê±° (ìµœì†Œí™”ëœ ë°ì´í„° COPYë§Œ)
    trigger_redshift_dag = TriggerDagRunOperator(
        task_id="trigger_redshift_dag",
        trigger_dag_id="redshift_s3_copy_minimal",
        conf={
            'job_id': "{{ dag_run.conf.get('job_id') if dag_run and dag_run.conf else run_id }}",
            'execution_time': "{{ ti.xcom_pull(task_ids='call_crawler', key='crawler_execution_time') }}",
            'dag_run_id': "{{ dag_run.run_id }}",
            'source_dag': 'realtime_pipeline_monitor',
            'trigger_point': 'call_crawler'
        },
        wait_for_completion=False,  # ë¹„ë™ê¸° ì‹¤í–‰
        poke_interval=30,
        dag=dag
    )

    # 8. Summary Analysis DAG íŠ¸ë¦¬ê±° (ë³‘ë ¬ ì‹¤í–‰)
    trigger_summary_dag = TriggerDagRunOperator(
        task_id="trigger_summary_dag",
        trigger_dag_id="summary_analysis_dag",
        conf={
            'job_id': "{{ dag_run.conf.get('job_id') if dag_run and dag_run.conf else run_id }}",
            'execution_time': "{{ ti.xcom_pull(task_ids='call_crawler', key='crawler_execution_time') }}",
            'copy_completion_time': "{{ ti.xcom_pull(task_ids='call_crawler', key='crawler_execution_time') }}",
            'source_dag': 'realtime_pipeline_monitor',
            'trigger_point': 'pipeline_completed',
            'redshift_copy_completed': False  # ì•„ì§ Redshift COPYëŠ” ì™„ë£Œë˜ì§€ ì•ŠìŒ
        },
        wait_for_completion=False,  # ë¹„ë™ê¸° ì‹¤í–‰
        poke_interval=30,
        dag=dag
    )

    # ì‘ì—… ìˆœì„œ ì •ì˜ (ë³‘ë ¬ ì²˜ë¦¬ í¬í•¨)
    call_crawler >> wait_collection
    wait_collection >> [wait_transform, wait_analysis, wait_aggregation]
    wait_aggregation >> [trigger_redshift_dag, trigger_summary_dag]

"""
DAG ì‹¤í–‰ ë°©ë²•:

1. ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§ (ê¸°ë³¸):
   airflow dags trigger realtime_pipeline_monitor --conf '{
     "job_id": "job-2024-001",
     "url": "https://example.com/product/123",
     "review_cnt": 100
   }'

1-1. ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§ (product_id í¬í•¨):
   airflow dags trigger realtime_pipeline_monitor --conf '{
     "job_id": "job-2024-001",
     "product_id": "product-123",
     "url": "https://example.com/product/123",
     "review_cnt": 100
   }'

2. ë‹¤ì¤‘ ìƒí’ˆ í¬ë¡¤ë§:
   airflow dags trigger realtime_pipeline_monitor --conf '{
     "job_id": "job-2024-002",
     "url_list": [
       "https://example.com/product/123",
       "https://example.com/product/456",
       "https://example.com/product/789"
     ]
   }'

3. Airflow UIì—ì„œ ìˆ˜ë™ ì‹¤í–‰:
   - DAG í˜ì´ì§€ì—ì„œ "Trigger DAG w/ Config" í´ë¦­
   - Configuration JSONì— ìœ„ ì˜ˆì‹œ ì¤‘ í•˜ë‚˜ ì…ë ¥

4. ì™¸ë¶€ APIë¡œ ì‹¤í–‰:
   curl -X POST "http://airflow-server:8080/api/v1/dags/realtime_pipeline_monitor/dagRuns" \
        -H "Content-Type: application/json" \
        -d '{
          "conf": {
            "job_id": "job-2024-001",
            "url": "https://example.com/product/123",
            "review_cnt": 100
          }
        }'

API ì—”ë“œí¬ì¸íŠ¸ ìë™ ì„ íƒ:
- url_listê°€ ìˆìœ¼ë©´ â†’ /crawl/product_multi (ë‹¤ì¤‘ ìƒí’ˆ í¬ë¡¤ë§)
- urlì´ ìˆìœ¼ë©´ â†’ /crawl/product_one (ë‹¨ì¼ ìƒí’ˆ í¬ë¡¤ë§)

ì£¼ì˜ì‚¬í•­:
- job_idëŠ” Control í† í”½ì˜ ë©”ì‹œì§€ì™€ ì¼ì¹˜í•´ì•¼ í•¨
- ê° ë‹¨ê³„ë³„ íƒ€ì„ì•„ì›ƒ: Collection(60ë¶„), Transform(30ë¶„), Analysis(45ë¶„), Aggregation(15ë¶„)
- ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ì—ëŸ¬ ë¡œê¹… ë° ì•Œë¦¼ ì²˜ë¦¬
- í¬ë¡¤ëŸ¬ ì„œë²„ ì—°ê²° IDëŠ” 'crawler_server'ë¡œ ì„¤ì • í•„ìš”
"""
