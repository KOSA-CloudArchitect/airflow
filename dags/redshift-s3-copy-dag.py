from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.providers.amazon.aws.operators.redshift_data import RedshiftDataOperator
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
import boto3
import json
import gzip
import pytz
from typing import List, Dict, Any

# Í∏∞Î≥∏ ÏÑ§Ï†ï
DAG_ID = 'redshift_s3_copy_pipeline'
S3_BUCKET = 'hihypipe-raw-data'
S3_PREFIX = 'topics/review-rows'
REDSHIFT_SCHEMA = 'public'
REDSHIFT_TABLE = 'realtime_review_collection'
REDSHIFT_DATABASE = 'hihypipe'

# Í∏∞Î≥∏ DAG Ïù∏Ïàò
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'catchup': False
}

# DAG Ï†ïÏùò (Ìä∏Î¶¨Í±∞ Í∏∞Î∞ò)
dag = DAG(
    DAG_ID,
    default_args=default_args,
    description='Redshift S3 COPY Pipeline for Review Data (Triggered)',
    schedule=None,  # Ìä∏Î¶¨Í±∞ Í∏∞Î∞ò Ïã§Ìñâ
    max_active_runs=1,
    tags=['redshift', 's3', 'kafka', 'review-data', 'triggered']
)

def extract_trigger_data(**context) -> Dict[str, Any]:
    """Ìä∏Î¶¨Í±∞ DAGÏóêÏÑú Ï†ÑÎã¨Î∞õÏùÄ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú"""
    dag_run = context['dag_run']
    conf = dag_run.conf if dag_run.conf else {}
    
    job_id = conf.get('job_id', 'unknown')
    execution_time_str = conf.get('execution_time', context['execution_date'].isoformat())
    source_dag = conf.get('source_dag', 'unknown')
    
    try:
        # ISO ÌòïÏãù ÏãúÍ∞ÑÏùÑ datetimeÏúºÎ°ú Î≥ÄÌôò
        execution_time = datetime.fromisoformat(execution_time_str.replace('Z', '+00:00'))
    except:
        execution_time = context['execution_date']
    
    trigger_data = {
        'job_id': job_id,
        'execution_time': execution_time,
        'execution_time_str': execution_time_str,
        'source_dag': source_dag
    }
    
    print(f"[Trigger Data] Extracted: {trigger_data}")
    return trigger_data

def extract_job_id_from_s3_file(s3_key: str) -> str:
    """S3 ÌååÏùºÏóêÏÑú job_id Ï∂îÏ∂ú"""
    s3_client = boto3.client('s3')
    try:
        response = s3_client.get_object(Bucket=S3_BUCKET, Key=s3_key)
        
        with gzip.GzipFile(fileobj=response['Body']) as gz_file:
            first_line = gz_file.readline()
            data = json.loads(first_line)
            return data.get('job_id', 'unknown')
    except Exception as e:
        print(f"Error extracting job_id from {s3_key}: {e}")
        return 'unknown'

def get_s3_files_by_job_and_time(**context) -> List[str]:
    """ÌäπÏ†ï job_idÏôÄ ÏãúÍ∞Ñ Ïù¥ÌõÑÏùò S3 ÌååÏùº Î™©Î°ùÏùÑ Í∞ÄÏ†∏Ïò§Îäî Ìï®Ïàò"""
    s3_client = boto3.client('s3')
    
    # Ìä∏Î¶¨Í±∞ Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞
    trigger_data = context['task_instance'].xcom_pull(task_ids='extract_trigger_data')
    job_id = trigger_data['job_id']
    execution_time = trigger_data['execution_time']
    
    print(f"[S3 Filter] Looking for files with job_id='{job_id}' after {execution_time}")
    
    # ÎÇ†Ïßú Ï∂îÏ∂ú (YYYYMMDD ÌòïÏãù, KST Í∏∞Ï§Ä) Î∞è ÌååÌã∞ÏÖò Ï†ëÎëêÏÇ¨ Íµ¨ÏÑ±
    kst = pytz.timezone('Asia/Seoul')
    execution_date = execution_time.astimezone(kst).strftime('%Y%m%d')
    prefix = f"{S3_PREFIX}/{execution_date}/partition=0/"
    
    files = []
    response = s3_client.list_objects_v2(Bucket=S3_BUCKET, Prefix=prefix)
    
    if 'Contents' in response:
        for obj in response['Contents']:
            if obj['Key'].endswith('.json.gz'):
                last_modified = obj['LastModified']
                
                # ÏãúÍ∞Ñ Í∏∞Ï§Ä ÌïÑÌÑ∞ÎßÅ
                if last_modified >= execution_time:
                    # ÌååÏùº ÎÇ¥Ïö©ÏóêÏÑú job_id ÌôïÏù∏
                    file_job_id = extract_job_id_from_s3_file(obj['Key'])
                    
                    # job_id Í∏∞Ï§Ä ÌïÑÌÑ∞ÎßÅ
                    if file_job_id == job_id:
                        files.append({
                            's3_path': f"s3://{S3_BUCKET}/{obj['Key']}",
                            'last_modified': last_modified,
                            'size': obj['Size'],
                            'job_id': file_job_id
                        })
                        print(f"[S3 Filter] Added file: {obj['Key']} (job_id: {file_job_id}, modified: {last_modified})")
                    else:
                        print(f"[S3 Filter] Skipped file: {obj['Key']} (job_id: {file_job_id} != {job_id})")
                else:
                    print(f"[S3 Filter] Skipped file: {obj['Key']} (too old: {last_modified} < {execution_time})")
    
    # ÌååÏùº ÌÅ¨Í∏∞Ïàú Ï†ïÎ†¨
    files.sort(key=lambda x: x['size'], reverse=True)
    
    print(f"[S3 Filter] Found {len(files)} files for job_id '{job_id}'")
    return [file['s3_path'] for file in files]

def get_latest_s3_files(**context) -> List[str]:
    """S3ÏóêÏÑú ÏµúÏã† ÌååÏùº Î™©Î°ùÏùÑ Í∞ÄÏ†∏Ïò§Îäî Ìï®Ïàò"""
    s3_client = boto3.client('s3')
    execution_date = context['ts']  # ISO with timezone
    dt = datetime.fromisoformat(execution_date.replace('Z', '+00:00'))
    kst = pytz.timezone('Asia/Seoul')
    korean_date = dt.astimezone(kst).strftime('%Y%m%d')
    
    # S3 Í≤ΩÎ°ú: topics/review-rows/YYYYMMDD/partition=0/
    prefix = f"{S3_PREFIX}/{korean_date}/partition=0/"
    
    try:
        response = s3_client.list_objects_v2(
            Bucket=S3_BUCKET,
            Prefix=prefix
        )
        
        files = []
        if 'Contents' in response:
            for obj in response['Contents']:
                if obj['Key'].endswith('.json.gz'):
                    files.append(f"s3://{S3_BUCKET}/{obj['Key']}")
        
        print(f"Found {len(files)} files for date {korean_date}")
        return files
        
    except Exception as e:
        print(f"Error listing S3 files: {e}")
        return []

def create_redshift_copy_sql_for_job(**context) -> str:
    """ÌäπÏ†ï job_idÏùò ÌååÏùºÎì§Îßå COPYÌïòÎäî SQL ÏÉùÏÑ±"""
    trigger_data = context['task_instance'].xcom_pull(task_ids='extract_trigger_data')
    job_id = trigger_data['job_id']
    files = context['task_instance'].xcom_pull(task_ids='get_s3_files_by_job_and_time')
    
    if not files:
        return f"-- No files found for job_id: {job_id}"
    
    file_list = "', '".join(files)
    
    return f"""
    -- Job ID: {job_id}
    -- Files: {len(files)}
    COPY {REDSHIFT_SCHEMA}.{REDSHIFT_TABLE} (
        review_id, job_id, product_id, title, tag, rating, review_count,
        sales_price, final_price, review_rating, review_date, review_text,
        clean_text, keywords, review_help_count, is_coupang_trial,
        is_empty_review, is_valid_rating, is_valid_date, has_content,
        is_valid, invalid_reason, year, month, day, quarter, yyyymm,
        yyyymmdd, weekday, summary, sentiment, crawled_at
    )
    FROM ('{file_list}')
    IAM_ROLE 'arn:aws:iam::ACCOUNT_ID:role/RedshiftRole'
    JSON 'auto'
    GZIP
    COMPUPDATE OFF
    STATUPDATE OFF
    TRUNCATECOLUMNS
    IGNOREHEADER 0
    DELIMITER ','
    ESCAPE
    NULL AS '\\N'
    EMPTYASNULL
    BLANKSASNULL
    DATEFORMAT 'auto'
    TIMEFORMAT 'auto'
    ACCEPTINVCHARS
    ACCEPTANYDATE
    IGNOREBLANKLINES
    TRIMBLANKS
    FILLRECORD
    MAXERROR 1000
    REGION 'ap-northeast-2';
    """

def validate_copy_results(**context) -> Dict[str, Any]:
    """COPY Í≤∞Í≥º Í≤ÄÏ¶ù"""
    redshift_hook = context['task_instance'].xcom_pull(task_ids='copy_to_redshift')
    
    # Í∏∞Î≥∏ Í≤ÄÏ¶ù Î°úÏßÅ
    validation_results = {
        'total_records': 0,
        'valid_records': 0,
        'invalid_records': 0,
        'validation_passed': False
    }
    
    try:
        # Ïó¨Í∏∞Ïóê Ïã§Ï†ú Í≤ÄÏ¶ù Î°úÏßÅ Íµ¨ÌòÑ
        validation_results['validation_passed'] = True
        print("Data validation completed successfully")
        
    except Exception as e:
        print(f"Validation error: {e}")
        validation_results['validation_passed'] = False
    
    return validation_results

# 1. Ìä∏Î¶¨Í±∞ Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
extract_trigger_data_task = PythonOperator(
    task_id='extract_trigger_data',
    python_callable=extract_trigger_data,
    dag=dag
)

# 2. S3 ÌååÏùº Î™©Î°ù Í∞ÄÏ†∏Ïò§Í∏∞ (job_idÏôÄ ÏãúÍ∞Ñ Í∏∞Ï§Ä ÌïÑÌÑ∞ÎßÅ)
get_s3_files = PythonOperator(
    task_id='get_s3_files_by_job_and_time',
    python_callable=get_s3_files_by_job_and_time,
    dag=dag
)

# 3. Î™®Îì† ÌÖåÏù¥Î∏î ÏÉùÏÑ± (ÌÜµÌï©)
create_all_tables = RedshiftDataOperator(
    task_id='create_all_tables',
    sql=f"""
    -- Î©îÏù∏ ÌÖåÏù¥Î∏î ÏÉùÏÑ±
    CREATE TABLE IF NOT EXISTS {REDSHIFT_SCHEMA}.{REDSHIFT_TABLE} (
        review_id VARCHAR(255) NOT NULL,
        job_id VARCHAR(255),
        product_id VARCHAR(255),
        title VARCHAR(MAX),
        tag VARCHAR(255),
        rating DECIMAL(3,2),
        review_count INTEGER,
        sales_price INTEGER,
        final_price INTEGER,
        review_rating DECIMAL(3,2),
        review_date TIMESTAMP,
        review_text VARCHAR(MAX),
        clean_text VARCHAR(MAX),
        keywords VARCHAR(MAX),
        review_help_count INTEGER,
        is_coupang_trial INTEGER,
        is_empty_review INTEGER,
        is_valid_rating INTEGER,
        is_valid_date INTEGER,
        has_content INTEGER,
        is_valid INTEGER,
        invalid_reason VARCHAR(MAX),
        year INTEGER,
        month INTEGER,
        day INTEGER,
        quarter INTEGER,
        yyyymm VARCHAR(6),
        yyyymmdd VARCHAR(8),
        weekday VARCHAR(10),
        summary VARCHAR(MAX),
        sentiment VARCHAR(50),
        crawled_at TIMESTAMP NOT NULL,
        loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    DISTKEY(review_id)
    SORTKEY(crawled_at, review_id);

    -- ÌÇ§ÏõåÎìú ÌÖåÏù¥Î∏î ÏÉùÏÑ±
    CREATE TABLE IF NOT EXISTS {REDSHIFT_SCHEMA}.review_keywords (
        review_id VARCHAR(255) NOT NULL,
        keyword_type VARCHAR(100),
        keyword_value VARCHAR(255),
        crawled_at TIMESTAMP,
        PRIMARY KEY (review_id, keyword_type)
    )
    DISTKEY(review_id)
    SORTKEY(review_id, keyword_type);

    -- Î¨¥Ìö® ÏÇ¨Ïú† ÌÖåÏù¥Î∏î ÏÉùÏÑ±
    CREATE TABLE IF NOT EXISTS {REDSHIFT_SCHEMA}.review_invalid_reasons (
        review_id VARCHAR(255) NOT NULL,
        reason_order INTEGER,
        reason_value VARCHAR(255),
        crawled_at TIMESTAMP,
        PRIMARY KEY (review_id, reason_order)
    )
    DISTKEY(review_id)
    SORTKEY(review_id, reason_order);
    """,
    database=REDSHIFT_DATABASE,
    workgroup_name='hihypipe-redshift-workgroup',
    aws_conn_id='aws_default',
    dag=dag
)

# S3ÏóêÏÑú RedshiftÎ°ú Îç∞Ïù¥ÌÑ∞ Î≥µÏÇ¨ (job_id Í∏∞Ï§Ä)
copy_to_redshift = RedshiftDataOperator(
    task_id='copy_to_redshift',
    sql=create_redshift_copy_sql_for_job,
    database=REDSHIFT_DATABASE,
    workgroup_name='hihypipe-redshift-workgroup',
    aws_conn_id='aws_default',
    dag=dag
)

# 5. JSON Îç∞Ïù¥ÌÑ∞ ÌååÏã± Î∞è Ïù∏Îç±Ïä§ ÏÉùÏÑ± (Î≥ëÎ†¨ Ï≤òÎ¶¨)
parse_json_data = RedshiftDataOperator(
    task_id='parse_json_data',
    sql=f"""
    -- JSON ÌååÏã± Ìï®Ïàò ÏÉùÏÑ±
    CREATE OR REPLACE FUNCTION parse_keywords(review_id VARCHAR(255), keywords_json TEXT)
    RETURNS VOID AS $$
    DECLARE
        keyword_record RECORD;
    BEGIN
        FOR keyword_record IN 
            SELECT * FROM json_each_text(keywords_json::json)
        LOOP
            INSERT INTO {REDSHIFT_SCHEMA}.review_keywords (review_id, keyword_type, keyword_value, crawled_at)
            VALUES (review_id, keyword_record.key, keyword_record.value, CURRENT_TIMESTAMP)
            ON CONFLICT (review_id, keyword_type) DO UPDATE SET
                keyword_value = EXCLUDED.keyword_value,
                crawled_at = EXCLUDED.crawled_at;
        END LOOP;
    END;
    $$ LANGUAGE plpgsql;
    
    CREATE OR REPLACE FUNCTION parse_invalid_reasons(review_id VARCHAR(255), reasons_json TEXT)
    RETURNS VOID AS $$
    DECLARE
        reason_record RECORD;
        reason_order INTEGER := 1;
    BEGIN
        FOR reason_record IN 
            SELECT * FROM json_array_elements_text(reasons_json::json)
        LOOP
            INSERT INTO {REDSHIFT_SCHEMA}.review_invalid_reasons (review_id, reason_order, reason_value, crawled_at)
            VALUES (review_id, reason_order, reason_record.value, CURRENT_TIMESTAMP)
            ON CONFLICT (review_id, reason_order) DO UPDATE SET
                reason_value = EXCLUDED.reason_value,
                crawled_at = EXCLUDED.crawled_at;
            reason_order := reason_order + 1;
        END LOOP;
    END;
    $$ LANGUAGE plpgsql;
    
    -- keywords ÌååÏã± Ïã§Ìñâ
    SELECT parse_keywords(review_id, keywords) 
    FROM {REDSHIFT_SCHEMA}.{REDSHIFT_TABLE} 
    WHERE keywords IS NOT NULL AND keywords != '{{}}' AND keywords != 'null';
    
    -- invalid_reason ÌååÏã± Ïã§Ìñâ
    SELECT parse_invalid_reasons(review_id, invalid_reason) 
    FROM {REDSHIFT_SCHEMA}.{REDSHIFT_TABLE} 
    WHERE invalid_reason IS NOT NULL AND invalid_reason != '[]' AND invalid_reason != 'null';
    """,
    database=REDSHIFT_DATABASE,
    workgroup_name='hihypipe-redshift-workgroup',
    aws_conn_id='aws_default',
    dag=dag
)

# 6. ÏÑ±Îä• ÏµúÏ†ÅÌôî Ïù∏Îç±Ïä§ ÏÉùÏÑ± (Î≥ëÎ†¨ Ï≤òÎ¶¨)
create_indexes = RedshiftDataOperator(
    task_id='create_indexes',
    sql=f"""
    -- Î©îÏù∏ ÌÖåÏù¥Î∏î Ïù∏Îç±Ïä§ (job_id Ïö∞ÏÑ†)
    CREATE INDEX IF NOT EXISTS idx_realtime_review_collection_job_id 
    ON {REDSHIFT_SCHEMA}.{REDSHIFT_TABLE} (job_id);
    
    CREATE INDEX IF NOT EXISTS idx_realtime_review_collection_is_valid 
    ON {REDSHIFT_SCHEMA}.{REDSHIFT_TABLE} (is_valid);
    
    CREATE INDEX IF NOT EXISTS idx_realtime_review_collection_sentiment 
    ON {REDSHIFT_SCHEMA}.{REDSHIFT_TABLE} (sentiment);
    
    CREATE INDEX IF NOT EXISTS idx_realtime_review_collection_yyyymm 
    ON {REDSHIFT_SCHEMA}.{REDSHIFT_TABLE} (yyyymm);
    
    CREATE INDEX IF NOT EXISTS idx_realtime_review_collection_quarter 
    ON {REDSHIFT_SCHEMA}.{REDSHIFT_TABLE} (quarter);
    
    -- Î≥µÌï© Ïù∏Îç±Ïä§ (job_id Í∏∞Î∞ò ÏøºÎ¶¨ ÏµúÏ†ÅÌôî)
    CREATE INDEX IF NOT EXISTS idx_realtime_review_collection_job_analysis 
    ON {REDSHIFT_SCHEMA}.{REDSHIFT_TABLE} (job_id, is_valid, sentiment);
    
    CREATE INDEX IF NOT EXISTS idx_realtime_review_collection_job_time 
    ON {REDSHIFT_SCHEMA}.{REDSHIFT_TABLE} (job_id, crawled_at);
    
    -- ÌÇ§ÏõåÎìú ÌÖåÏù¥Î∏î Ïù∏Îç±Ïä§
    CREATE INDEX IF NOT EXISTS idx_review_keywords_keyword_type 
    ON {REDSHIFT_SCHEMA}.review_keywords (keyword_type);
    
    CREATE INDEX IF NOT EXISTS idx_review_keywords_keyword_value 
    ON {REDSHIFT_SCHEMA}.review_keywords (keyword_value);
    """,
    database=REDSHIFT_DATABASE,
    workgroup_name='hihypipe-redshift-workgroup',
    aws_conn_id='aws_default',
    dag=dag
)

# 7. Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù Î∞è ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏ (ÌÜµÌï©)
validate_and_update_stats = PythonOperator(
    task_id='validate_and_update_stats',
    python_callable=lambda **context: (
        validate_copy_results(**context),
        # ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏ÎèÑ Ìï®Íªò Ïã§Ìñâ
        print("üìä Updating table statistics...", flush=True)
    )[0],  # Í≤ÄÏ¶ù Í≤∞Í≥º Î∞òÌôò
    dag=dag
)

# ÏûëÏóÖ ÏàúÏÑú Ï†ïÏùò (Î≥ëÎ†¨ Ï≤òÎ¶¨ Ìè¨Ìï®)
extract_trigger_data_task >> get_s3_files >> create_all_tables >> copy_to_redshift >> [parse_json_data, create_indexes] >> validate_and_update_stats
